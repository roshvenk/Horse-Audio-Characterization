import librosa
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import os
from PIL import Image
import os, time, warnings
import pathlib
import csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import keras
from keras import layers
import keras
from keras.models import Sequential
from tensorflow.keras.utils import to_categorical
import warnings
warnings.filterwarnings('ignore')
from tensorflow.keras.layers import (
    Dense,
    Conv1D,
    MaxPooling1D,
    BatchNormalization,
    Dropout,
    Flatten,
    Conv2D,
    MaxPool2D,
)
from sklearn import linear_model
from sklearn import metrics

cmap = plt.get_cmap('inferno')
plt.figure(figsize=(8,8))
audiofiles = '0001 0002 0003 0005 0006 0007 0008 0009 0010 0011 0012 0013 0014 0016 0017 0018 0019 0021 0022 0023 0024 0025 0026 0027 0028 0029 0030 0033 0036 0038 0039 0040 0041 0043 0044 R0001 R0002 R0003 R0004 R0005 R0006'.split()
for g in audiofiles: 
    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)
    for filename in os.listdir(f'./drive/MyDrive/Colab Notebooks/Audio/{g}'):
        songname = f'./drive/MyDrive/Colab Notebooks/Audio/{g}/{filename}'
        y, sr = librosa.load(songname, mono=True, duration=5)
        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');
        plt.axis('off');
        plt.savefig(f'img_data/{g}/{filename[:-3].replace(".", "")}.png')
        plt.clf()
        
header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'
for i in range(1, 21):
    header += f' mfcc{i}'
header += ' label'
header = header.split()

file = open('dataset.csv', 'w', newline='')
with file:
    writer = csv.writer(file)
    writer.writerow(header)
audiofiles = '0001 0002 0003 0005 0006 0007 0008 0009 0010 0011 0012 0013 0014 0016 0017 0018 0019 0021 0022 0023 0024 0025 0026 0027 0028 0029 0030 0033 0036 0038 0039 0040 0041 0043 0044 R0001 R0002 R0003 R0004 R0005 R0006'.split()

for g in audiofiles:
    all_files = os.listdir(f'./drive/MyDrive/Colab Notebooks/Audio/{g}')
    count = 0
    main_ls = []
    for filename in all_files:            
        songname = f'./drive/MyDrive/Colab Notebooks/Audio/{g}/{filename}'
        y, sr = librosa.load(songname, mono=True, duration=30)
        rmse=librosa.feature.rms(y=y)[0]
        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)
        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)
        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        zcr = librosa.feature.zero_crossing_rate(y)
        mfcc = librosa.feature.mfcc(y=y, sr=sr)
        to_append = [f'{g}'] + [np.mean(chroma_stft)] + [np.mean(rmse)] + [np.mean(spec_cent)] + [np.mean(spec_bw)] + [np.mean(rolloff)] + [np.mean(zcr)]    
        for e in mfcc:
            to_append += [np.mean(e)]
        
        if count == 0:
            main_ls = to_append.copy()
        

        else:
            for i in range(len(main_ls)):
                if type(main_ls[i]) != str:
                    main_ls[i] += to_append[i]
        
        if count == len(all_files) - 1:
            for i in range(len(main_ls)):
                if type(main_ls[i]) != str:
                    main_ls[i] /= len(all_files)

            file = open('dataset.csv', 'a', newline='')
            with file:
                writer = csv.writer(file)
                writer.writerow(main_ls)

        count += 1
        
        
import pandas as pd

file1 = "./drive/MyDrive/Colab Notebooks/Clinical report summary1.csv"
file2 = "./drive/MyDrive/Colab Notebooks/dataset (3).csv"
df1 = pd.read_csv(file1)
df2 = pd.read_csv(file2)

req_col = df1["Score (By Roshan)"]
df1_req = req_col.to_frame()
final = pd.concat([df2, df1_req], axis = 1)

final.to_csv("out.csv", sep=',', encoding='utf-8',index = False)


df = pd.read_csv('./drive/MyDrive/Colab Notebooks/out.csv')
column_names = list(df.columns)
input_names = column_names[1:len(column_names)-1]
target_name = column_names[len(column_names)-1]
X = df[input_names]      # slice dataFrame to extract input variables
y = df[target_name]        # slice dataFrame to extract target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
clf = linear_model.LogisticRegression(solver='liblinear').fit(X_train, y_train)


y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)
# Let's examine one sample and the corresponding predictions
print('----- Sample case -----')
last_sample = X_test.loc[list(X_test.index)[-1]]
print(last_sample)
last_sample_proba = y_pred_proba[-1]
print('Predicted class:',y_pred[-1])
print('Actual class:', y_test.loc[list(y_test.index)[-1]])

print("Accuracy:", metrics.accuracy_score(y_test, y_pred))

data = pd.read_csv('./drive/MyDrive/Colab Notebooks/out.csv')
data.head()# Dropping unneccesary columns
data = data.drop(['filename'],axis=1)#Encoding the Labels
genre_list = data.iloc[:, -1]
encoder = LabelEncoder()
y = encoder.fit_transform(genre_list)#Scaling the Feature columns
scaler = StandardScaler()
X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))#Dividing data into training and Testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

data = pd.read_csv('./drive/MyDrive/Colab Notebooks/out.csv')
data.head()# Dropping unneccesary columns
data = data.drop(['filename'],axis=1)#Encoding the Labels
genre_list = data.iloc[:, -1]
encoder = LabelEncoder()
y = encoder.fit_transform(genre_list)#Scaling the Feature columns
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
scaler = DecisionTreeClassifier()
scaler= scaler.fit(X_train, y_train)
X = scaler.fit(X_train,y_train)#Dividing data into training and Testing set
y_pred = scaler.predict(X_test)

print("accuracy:", metrics.accuracy_score(y_test, y_pred))

data = pd.read_csv('./drive/MyDrive/Colab Notebooks/out.csv')
encoder = LabelEncoder()
y = encoder.fit_transform(genre_list)#Scaling the Feature columns
scaler2 = RandomForestClassifier(n_estimators = 100)
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X = scaler2.fit(X_train,y_train)#Dividing data into training and Testing set
y_pred = scaler2.predict(X_test)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))

extracted = pd.read_csv("./drive/MyDrive/Colab Notebooks/out.csv")

# create a new dataframe
extracted_df = pd.DataFrame(extracted)
# Storing the dataframe to pickle for further processing
extracted_df.to_pickle("extracted_df.pkl")
extracted_df.head()

final = pd.DataFrame(extracted)
X = (final[["chroma_stft", "rmse", "spectral_centroid", "spectral_bandwidth", "rolloff", "zero_crossing_rate", "mfcc1", "mfcc2", "mfcc3", "mfcc4", "mfcc5", "mfcc6", "mfcc7", "mfcc8", "mfcc9", "mfcc10", "mfcc11", "mfcc12", "mfcc13", "mfcc14", "mfcc15", "mfcc16", "mfcc17", "mfcc18", "mfcc19", "mfcc20"]])
print(X)
y = (final["Score (By Roshan)"])

# label encoding to get encoding
le = LabelEncoder()

# transform each category with it's respected label
Y = to_categorical(le.fit_transform(y))

# split the data to train and test set
X_train, X_test, y_train, y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42
)

# print the details
print("Number of training samples = ", X_train.shape[0])
print("Number of testing samples = ", X_test.shape[0])
print(X_test)
print(y_test)

num_labels = Y.shape[1]
ANN_Model = Sequential()
ANN_Model.add(Dense(1000, activation="relu", input_shape=(None,32,26)))
ANN_Model.add(Dense(750, activation="relu"))
ANN_Model.add(Dense(500, activation="relu"))
ANN_Model.add(Dense(250, activation="relu"))
ANN_Model.add(Dense(100, activation="relu"))
ANN_Model.add(Dense(50, activation="relu"))
ANN_Model.add(Dense(num_labels, activation="softmax"))
ANN_Model.summary()

ANN_Model.compile(
    optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"]
)

num_epochs = 250
num_batch_size = 32

t0 = time.time()

ANN_Results = ANN_Model.fit(
    X_train,
    y_train,
    batch_size=num_batch_size,
    epochs=num_epochs,
    validation_data=(X_test, y_test),
)

ANN_Model.save("Model1.h5")
print("ANN Model Saved")
train_hist_m1 = pd.DataFrame(ANN_Results.history)
train_m1 = round(time.time() - t0, 3)

plt.figure(figsize=(10, 5), dpi=300)
plt.plot(train_hist_m1[["loss", "val_loss"]])
plt.legend(["Loss", "Validation Loss"])
plt.title("Loss Per Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()
